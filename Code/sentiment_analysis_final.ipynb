{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK11_i2Lu7Yp",
        "outputId": "4185b06d-2df6-4a97-ff85-5df4d7a99967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                            sentence sentiment\n",
            "0  according to gran  the company has no plans to...   neutral\n",
            "1  technopolis plans to develop in stages an area...   neutral\n",
            "2  the international electronic industry company ...  negative\n",
            "3  with the new production plant the company woul...  positive\n",
            "4  according to the company s updated strategy fo...  positive\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Function to load and preprocess data from a file\n",
        "def load_and_preprocess(file_path, encoding):\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding=encoding) as file:\n",
        "        for line in file:\n",
        "            # Splitting the sentence and the sentiment label\n",
        "            sentence, sentiment = line.rsplit('@', 1)\n",
        "            # Normalizing text: converting to lowercase and removing special characters\n",
        "            sentence = re.sub(r'[^a-zA-Z0-9\\s]', '', sentence.lower())\n",
        "            data.append({'sentence': sentence, 'sentiment': sentiment.strip()})\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "file_path_50 = 'Sentences_50Agree.txt'\n",
        "#file_path_all = 'Sentences_AllAgree.txt'\n",
        "\n",
        "encoding_50 = 'ISO-8859-1'\n",
        "#encoding_all = 'ISO-8859-1'\n",
        "\n",
        "# Loading and preprocessing data from both files\n",
        "data_50 = load_and_preprocess(file_path_50, encoding_50)\n",
        "#data_all = load_and_preprocess(file_path_all, encoding_all)\n",
        "\n",
        "\n",
        "print(data_50.head())\n",
        "#print(data_all.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5_ZUJClvunU",
        "outputId": "c2b57816-62f9-49aa-a213-8689eed60641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    00  000  000063  0008  001  002  0025  003  0030  004  ...  zinc  \\\n",
            "0  0.0  0.0     0.0   0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...   0.0   \n",
            "1  0.0  0.0     0.0   0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...   0.0   \n",
            "2  0.0  0.0     0.0   0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...   0.0   \n",
            "3  0.0  0.0     0.0   0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...   0.0   \n",
            "4  0.0  0.0     0.0   0.0  0.0  0.0   0.0  0.0   0.0  0.0  ...   0.0   \n",
            "\n",
            "   zinclead  zip  zloty  zoltan  zone  zoo  zte   zu  sentiment  \n",
            "0       0.0  0.0    0.0     0.0   0.0  0.0  0.0  0.0    neutral  \n",
            "1       0.0  0.0    0.0     0.0   0.0  0.0  0.0  0.0    neutral  \n",
            "2       0.0  0.0    0.0     0.0   0.0  0.0  0.0  0.0   negative  \n",
            "3       0.0  0.0    0.0     0.0   0.0  0.0  0.0  0.0   positive  \n",
            "4       0.0  0.0    0.0     0.0   0.0  0.0  0.0  0.0   positive  \n",
            "\n",
            "[5 rows x 11215 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize a TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the sentences from data_66\n",
        "tfidf_matrix = vectorizer.fit_transform(data_50['sentence'])\n",
        "\n",
        "# Creating a DataFrame for the TF-IDF features\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Adding sentiment information back to the TF-IDF DataFrame\n",
        "tfidf_df['sentiment'] = data_50['sentiment']\n",
        "\n",
        "# Displaying the first few rows of the TF-IDF DataFrame\n",
        "print(tfidf_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TVc-s-g0FFMM"
      },
      "outputs": [],
      "source": [
        "X = tfidf_df.drop('sentiment', axis=1)\n",
        "y = tfidf_df['sentiment']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z46BO2IrxFxZ"
      },
      "source": [
        "# Logistic Regression (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVmZBiVOxFCc",
        "outputId": "854c27ec-8bbd-4cdb-997d-6acfdf9650ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.92      0.45      0.60       110\n",
            "     neutral       0.73      0.96      0.83       571\n",
            "    positive       0.80      0.47      0.60       289\n",
            "\n",
            "    accuracy                           0.76       970\n",
            "   macro avg       0.82      0.63      0.68       970\n",
            "weighted avg       0.78      0.76      0.73       970\n",
            "\n",
            "Accuracy: 0.7556701030927835\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Assuming tfidf_df is your DataFrame from the previous TF-IDF transformation\n",
        "# X will contain the TF-IDF features and y will contain the sentiment labels\n",
        "X = tfidf_df.drop('sentiment', axis=1)\n",
        "y = tfidf_df['sentiment']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iicuolqXyK8E"
      },
      "source": [
        "# Logistic Regression (with hyperparameter tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_IdFJpWySKF",
        "outputId": "8afcdf0c-c2b4-4d78-f636-82e87dfd1585"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.74      0.64      0.68       110\n",
            "     neutral       0.80      0.87      0.83       571\n",
            "    positive       0.73      0.64      0.68       289\n",
            "\n",
            "    accuracy                           0.77       970\n",
            "   macro avg       0.76      0.71      0.73       970\n",
            "weighted avg       0.77      0.77      0.77       970\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],       # Norm used in the penalization\n",
        "    'solver': ['liblinear']         # Algorithm to use in the optimization problem\n",
        "}\n",
        "\n",
        "# Initialize the Grid Search with cross-validation\n",
        "grid_search = GridSearchCV(LogisticRegression(class_weight='balanced'), param_grid, cv=5, scoring='f1_macro')\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters found by GridSearchCV\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Train the model using the best parameters\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the best model\n",
        "print(classification_report(y_test, y_pred_best))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wr-ZLPLymps"
      },
      "source": [
        "# Ensemble Methods: RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD_tTncvyxC-",
        "outputId": "bd7e8ffd-6f09-44c5-c442-0b4b329fb997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.86      0.40      0.55       110\n",
            "     neutral       0.71      0.98      0.82       571\n",
            "    positive       0.85      0.39      0.54       289\n",
            "\n",
            "    accuracy                           0.74       970\n",
            "   macro avg       0.81      0.59      0.64       970\n",
            "weighted avg       0.77      0.74      0.71       970\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "random_forest = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "\n",
        "# Train the model\n",
        "random_forest.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_rf = random_forest.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llq72YW-zCO0"
      },
      "source": [
        "# Ensemble Methods: RandomForest (with hyperparameter tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANzpNfo3zH_R",
        "outputId": "65011b6a-89a6-4f91-97fe-1cdd3bbd4406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   1.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   1.3s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   4.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   3.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   3.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   4.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   3.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   7.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   6.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   7.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   8.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   7.2s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  15.2s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  15.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  14.4s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  15.1s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  15.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   3.4s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   4.3s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   3.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   3.4s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   4.1s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   7.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   7.4s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   6.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   7.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   7.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  13.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  14.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  13.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  14.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  15.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.7s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   3.4s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   4.2s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   3.3s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   3.4s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   3.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   6.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   7.3s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   6.4s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   7.4s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   6.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  14.3s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  13.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  13.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  14.2s\n",
            "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  14.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.7s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   1.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   1.1s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.7s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   3.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.7s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.3s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.7s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   6.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.1s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  11.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  11.1s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  10.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  11.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  11.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.7s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.7s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.7s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.7s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   3.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   3.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   2.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   2.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   2.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   6.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.1s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   6.1s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   5.3s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   6.2s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  11.3s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  10.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  11.1s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  10.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  11.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   1.1s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.7s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   2.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   2.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   2.7s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   3.4s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   2.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   6.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   5.3s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  11.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  10.7s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  10.7s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  11.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  11.1s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   2.2s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   2.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   2.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   2.1s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   2.2s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.7s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.3s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.3s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.7s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   8.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   2.1s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   2.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   2.1s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   2.2s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   2.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.3s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.4s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.7s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   2.2s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   2.1s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   2.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   2.1s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   2.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   4.6s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   7.8s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.4s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   8.5s\n",
            "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   7.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   1.2s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   1.0s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.7s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.7s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.7s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   1.7s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.4s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   3.1s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   3.1s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   3.1s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   3.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   3.1s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   1.0s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   1.5s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.7s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.7s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.7s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.7s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   3.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   3.2s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   3.1s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   3.1s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   3.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   1.7s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   2.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   3.0s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   3.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   3.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   3.1s\n",
            "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   3.1s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.7s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   2.2s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   3.1s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   3.1s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   3.0s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   3.8s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   3.0s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   1.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.8s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   3.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   3.4s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   3.0s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   3.1s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   3.8s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.8s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   1.5s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   2.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   1.7s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   3.0s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   3.0s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   3.7s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   3.1s\n",
            "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   3.0s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.0s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   1.5s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   1.5s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   1.5s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   1.5s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   3.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   2.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   2.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   2.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   3.7s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   2.2s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   1.8s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   3.0s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   3.2s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   3.4s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   3.1s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   2.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   1.3s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   1.2s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   2.1s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   3.2s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   3.0s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   3.0s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   3.8s\n",
            "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   3.0s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   1.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   1.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   1.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   1.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.9s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.0s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.9s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   6.2s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   5.6s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   6.2s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   5.6s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   6.2s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.6s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   1.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   1.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   1.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   2.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   2.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   2.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   5.3s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   6.2s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   5.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   6.3s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   5.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   2.1s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   1.7s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   1.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   1.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   2.7s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   2.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.6s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   2.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   2.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   6.1s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   5.2s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   6.0s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   5.1s\n",
            "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   5.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.3s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   2.6s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   2.6s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   2.6s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.1s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   5.2s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   5.0s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   5.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   5.1s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   5.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   1.6s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   1.9s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.6s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.6s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.2s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   2.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   5.0s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   5.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   5.0s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   5.7s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   5.0s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   1.3s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   2.1s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   1.3s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   2.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   2.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   2.7s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.0s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   2.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   4.7s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   5.7s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   4.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   5.7s\n",
            "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   4.9s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.3s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   2.0s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.3s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.3s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.3s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   2.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   2.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.1s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   2.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   2.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   4.6s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   5.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   4.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   5.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   4.7s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   1.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   1.6s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   1.3s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   1.3s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   2.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   2.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   2.9s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   2.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   2.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   4.6s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   5.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   4.8s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   5.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   4.6s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   1.3s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   1.9s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   1.3s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   1.3s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   2.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   2.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   2.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.0s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   2.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   4.6s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   5.4s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   4.7s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   5.5s\n",
            "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   4.8s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.6s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.6s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   2.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   2.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   2.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.4s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.2s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   7.4s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   7.8s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   7.2s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   2.1s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   2.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   2.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   1.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   1.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.6s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   7.3s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   7.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   7.3s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   8.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.6s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   2.2s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   2.4s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   2.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   2.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   2.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.6s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.1s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.8s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.1s\n",
            "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   7.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   1.8s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.2s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.1s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   1.8s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   1.8s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.4s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.4s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.4s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   6.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   7.3s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   6.6s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   7.3s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   6.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.8s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.8s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   1.8s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   1.8s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   1.8s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   1.8s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   1.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.3s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.3s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   3.4s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   7.1s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   6.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   7.1s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   6.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   7.1s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   1.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   1.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   2.4s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   1.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   1.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.3s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.3s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.3s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.3s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   7.2s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   6.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   7.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   7.3s\n",
            "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   6.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.4s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=10; total time=   0.6s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   2.3s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.6s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   1.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.4s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.4s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.1s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.1s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   6.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   6.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   6.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   6.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   6.3s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.8s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=10; total time=   0.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   1.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   1.6s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   1.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   1.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   1.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.1s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.2s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.1s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   5.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   6.2s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   7.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   8.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=10; total time=   0.5s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   1.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   1.6s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   1.7s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   1.6s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   2.4s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.1s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.1s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   6.6s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   6.2s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   9.0s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   5.9s\n",
            "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   7.0s\n",
            "Best parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.79      0.54      0.64       104\n",
            "     neutral       0.78      0.94      0.85       485\n",
            "    positive       0.77      0.58      0.66       255\n",
            "\n",
            "    accuracy                           0.78       844\n",
            "   macro avg       0.78      0.68      0.72       844\n",
            "weighted avg       0.78      0.78      0.77       844\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameters and their values for the grid search\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [10, 50, 100, 200],  # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20, 30],      # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5, 10],      # Minimum number of samples required to split a node\n",
        "    'min_samples_leaf': [1, 2, 4]         # Minimum number of samples required at each leaf node\n",
        "}\n",
        "\n",
        "# Initialize the Grid Search model\n",
        "grid_search_rf = GridSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42), param_grid_rf, cv=5, scoring='f1_macro', verbose=2)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters found by GridSearchCV\n",
        "print(\"Best parameters:\", grid_search_rf.best_params_)\n",
        "\n",
        "# Train the model using the best parameters\n",
        "best_rf_model = grid_search_rf.best_estimator_\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "y_pred_rf_best = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the best model\n",
        "print(classification_report(y_test, y_pred_rf_best))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9OaOMDiEO-U"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKJna6CaA2ag",
        "outputId": "62b4152f-8fb5-48b6-d283-e730558e7bdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default GPU Device: /device:GPU:0\n",
            "Epoch 1/10\n",
            "53/53 - 7s - loss: 0.8617 - accuracy: 0.6190 - 7s/epoch - 127ms/step\n",
            "Epoch 2/10\n",
            "53/53 - 0s - loss: 0.5397 - accuracy: 0.7631 - 223ms/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "53/53 - 0s - loss: 0.2299 - accuracy: 0.9244 - 213ms/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "53/53 - 0s - loss: 0.0744 - accuracy: 0.9810 - 207ms/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "53/53 - 0s - loss: 0.0294 - accuracy: 0.9932 - 211ms/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "53/53 - 0s - loss: 0.0127 - accuracy: 0.9964 - 210ms/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "53/53 - 0s - loss: 0.0080 - accuracy: 0.9991 - 209ms/epoch - 4ms/step\n",
            "Epoch 8/10\n",
            "53/53 - 0s - loss: 0.0056 - accuracy: 0.9988 - 204ms/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "53/53 - 0s - loss: 0.0057 - accuracy: 0.9985 - 202ms/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "53/53 - 0s - loss: 0.0030 - accuracy: 0.9991 - 202ms/epoch - 4ms/step\n",
            "Test Accuracy: 0.813\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Check if the GPU is available\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"GPU not found. Please select GPU as your runtime type.\")\n",
        "\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_y = label_encoder.fit_transform(y)  # Replace 'y' with your sentiment labels\n",
        "# Convert the encoded labels to one-hot-encoding\n",
        "y_cat = to_categorical(encoded_y)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train_cat, y_test_cat = train_test_split(X, y_cat, test_size=0.2, random_state=42)  # Replace 'X' with your features\n",
        "\n",
        "# Convert the DataFrame to a NumPy array\n",
        "X_train_np = np.array(X_train)\n",
        "X_test_np = np.array(X_test)\n",
        "\n",
        "# Define the neural network architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(X_train_np.shape[1],), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_np, y_train_cat, epochs=10, batch_size=64, verbose=2)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = model.evaluate(X_test_np, y_test_cat, verbose=0)\n",
        "print(f'Test Accuracy: {accuracy:.3f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG1YLl83ERs9"
      },
      "source": [
        "# NLP Approach - Using LLMs: BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE3T15u9Easm",
        "outputId": "a3e04b7a-83fd-46c1-cac9-c0fbbd322535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvZ1xs_WEcuv",
        "outputId": "f17489a6-0763-4ada-8ba3-9152d43a5fcf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "244/244 [==============================] - 230s 786ms/step - loss: 0.4713 - accuracy: 0.8091 - val_loss: 0.3537 - val_accuracy: 0.8629\n",
            "Epoch 2/2\n",
            "244/244 [==============================] - 188s 769ms/step - loss: 0.1252 - accuracy: 0.9572 - val_loss: 0.5394 - val_accuracy: 0.8505\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7aa21b9fa020>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming 'data' is your DataFrame with 'sentence' and 'sentiment' columns\n",
        "# Splitting the dataset into training and testing sets\n",
        "train_df, test_df = train_test_split(data_50, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load BERT tokenizer and BERT model\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "# Function to convert data to InputExamples\n",
        "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN):\n",
        "    train_InputExamples = train.apply(lambda x: InputExample(guid=None,\n",
        "                                                            text_a = x[DATA_COLUMN],\n",
        "                                                            text_b = None,\n",
        "                                                            label = x[LABEL_COLUMN]), axis = 1)\n",
        "    validation_InputExamples = test.apply(lambda x: InputExample(guid=None,\n",
        "                                                                text_a = x[DATA_COLUMN],\n",
        "                                                                text_b = None,\n",
        "                                                                label = x[LABEL_COLUMN]), axis = 1)\n",
        "    return train_InputExamples, validation_InputExamples\n",
        "\n",
        "# Function to convert InputExamples to TF Dataset\n",
        "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
        "    features = []\n",
        "\n",
        "    for e in examples:\n",
        "        input_dict = tokenizer.encode_plus(\n",
        "            e.text_a,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
        "            input_dict[\"token_type_ids\"], input_dict[\"attention_mask\"])\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def gen():\n",
        "        for f in features:\n",
        "            yield (\n",
        "                {\n",
        "                    \"input_ids\": f.input_ids,\n",
        "                    \"attention_mask\": f.attention_mask,\n",
        "                    \"token_type_ids\": f.token_type_ids,\n",
        "                },\n",
        "                f.label,\n",
        "            )\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
        "        (\n",
        "            {\n",
        "                \"input_ids\": tf.TensorShape([None]),\n",
        "                \"attention_mask\": tf.TensorShape([None]),\n",
        "                \"token_type_ids\": tf.TensorShape([None]),\n",
        "            },\n",
        "            tf.TensorShape([]),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "# Encode label column\n",
        "label_encoder = LabelEncoder()\n",
        "train_df['sentiment'] = label_encoder.fit_transform(train_df['sentiment'])\n",
        "test_df['sentiment'] = label_encoder.transform(test_df['sentiment'])\n",
        "\n",
        "# Preprocess and convert data\n",
        "DATA_COLUMN = 'sentence'\n",
        "LABEL_COLUMN = 'sentiment'\n",
        "train_InputExamples, validation_InputExamples = convert_data_to_examples(train_df, test_df, DATA_COLUMN, LABEL_COLUMN)\n",
        "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
        "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
        "\n",
        "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
        "validation_data = validation_data.batch(32)\n",
        "\n",
        "# Compile the BERT model\n",
        "bert_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08),\n",
        "                   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                   metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
        "\n",
        "# Train the BERT model\n",
        "bert_model.fit(train_data, epochs=2, validation_data=validation_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "hkhdQ9eHauet"
      },
      "outputs": [],
      "source": [
        "# Save the model's weights\n",
        "bert_saved_model = 'bert_model'\n",
        "bert_model.save_weights(bert_saved_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMRVgj4bWLKd"
      },
      "source": [
        "# Run model to make predictions on new observations (using BERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCiXwk7HWLdE",
        "outputId": "362b051f-9113-479b-d5ca-0770b320e330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting the sentiment analysis script...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the trained BERT model...\n",
            "Loading data from the JSON file...\n",
            "Initializing the BERT tokenizer...\n",
            "Running predictions on summaries...\n",
            "Starting sentiment prediction for each text...\n",
            "Processing text 1/850\n",
            "Processing text 2/850\n",
            "Processing text 3/850\n",
            "Processing text 4/850\n",
            "Processing text 5/850\n",
            "Processing text 6/850\n",
            "Processing text 7/850\n",
            "Processing text 8/850\n",
            "Processing text 9/850\n",
            "Processing text 10/850\n",
            "Processing text 11/850\n",
            "Processing text 12/850\n",
            "Processing text 13/850\n",
            "Processing text 14/850\n",
            "Processing text 15/850\n",
            "Processing text 16/850\n",
            "Processing text 17/850\n",
            "Processing text 18/850\n",
            "Processing text 19/850\n",
            "Processing text 20/850\n",
            "Processing text 21/850\n",
            "Processing text 22/850\n",
            "Processing text 23/850\n",
            "Processing text 24/850\n",
            "Processing text 25/850\n",
            "Processing text 26/850\n",
            "Processing text 27/850\n",
            "Processing text 28/850\n",
            "Processing text 29/850\n",
            "Processing text 30/850\n",
            "Processing text 31/850\n",
            "Processing text 32/850\n",
            "Processing text 33/850\n",
            "Processing text 34/850\n",
            "Processing text 35/850\n",
            "Processing text 36/850\n",
            "Processing text 37/850\n",
            "Processing text 38/850\n",
            "Processing text 39/850\n",
            "Processing text 40/850\n",
            "Processing text 41/850\n",
            "Processing text 42/850\n",
            "Processing text 43/850\n",
            "Processing text 44/850\n",
            "Processing text 45/850\n",
            "Processing text 46/850\n",
            "Processing text 47/850\n",
            "Processing text 48/850\n",
            "Processing text 49/850\n",
            "Processing text 50/850\n",
            "Processing text 51/850\n",
            "Processing text 52/850\n",
            "Processing text 53/850\n",
            "Processing text 54/850\n",
            "Processing text 55/850\n",
            "Processing text 56/850\n",
            "Processing text 57/850\n",
            "Processing text 58/850\n",
            "Processing text 59/850\n",
            "Processing text 60/850\n",
            "Processing text 61/850\n",
            "Processing text 62/850\n",
            "Processing text 63/850\n",
            "Processing text 64/850\n",
            "Processing text 65/850\n",
            "Processing text 66/850\n",
            "Processing text 67/850\n",
            "Processing text 68/850\n",
            "Processing text 69/850\n",
            "Processing text 70/850\n",
            "Processing text 71/850\n",
            "Processing text 72/850\n",
            "Processing text 73/850\n",
            "Processing text 74/850\n",
            "Processing text 75/850\n",
            "Processing text 76/850\n",
            "Processing text 77/850\n",
            "Processing text 78/850\n",
            "Processing text 79/850\n",
            "Processing text 80/850\n",
            "Processing text 81/850\n",
            "Processing text 82/850\n",
            "Processing text 83/850\n",
            "Processing text 84/850\n",
            "Processing text 85/850\n",
            "Processing text 86/850\n",
            "Processing text 87/850\n",
            "Processing text 88/850\n",
            "Processing text 89/850\n",
            "Processing text 90/850\n",
            "Processing text 91/850\n",
            "Processing text 92/850\n",
            "Processing text 93/850\n",
            "Processing text 94/850\n",
            "Processing text 95/850\n",
            "Processing text 96/850\n",
            "Processing text 97/850\n",
            "Processing text 98/850\n",
            "Processing text 99/850\n",
            "Processing text 100/850\n",
            "Processing text 101/850\n",
            "Processing text 102/850\n",
            "Processing text 103/850\n",
            "Processing text 104/850\n",
            "Processing text 105/850\n",
            "Processing text 106/850\n",
            "Processing text 107/850\n",
            "Processing text 108/850\n",
            "Processing text 109/850\n",
            "Processing text 110/850\n",
            "Processing text 111/850\n",
            "Processing text 112/850\n",
            "Processing text 113/850\n",
            "Processing text 114/850\n",
            "Processing text 115/850\n",
            "Processing text 116/850\n",
            "Processing text 117/850\n",
            "Processing text 118/850\n",
            "Processing text 119/850\n",
            "Processing text 120/850\n",
            "Processing text 121/850\n",
            "Processing text 122/850\n",
            "Processing text 123/850\n",
            "Processing text 124/850\n",
            "Processing text 125/850\n",
            "Processing text 126/850\n",
            "Processing text 127/850\n",
            "Processing text 128/850\n",
            "Processing text 129/850\n",
            "Processing text 130/850\n",
            "Processing text 131/850\n",
            "Processing text 132/850\n",
            "Processing text 133/850\n",
            "Processing text 134/850\n",
            "Processing text 135/850\n",
            "Processing text 136/850\n",
            "Processing text 137/850\n",
            "Processing text 138/850\n",
            "Processing text 139/850\n",
            "Processing text 140/850\n",
            "Processing text 141/850\n",
            "Processing text 142/850\n",
            "Processing text 143/850\n",
            "Processing text 144/850\n",
            "Processing text 145/850\n",
            "Processing text 146/850\n",
            "Processing text 147/850\n",
            "Processing text 148/850\n",
            "Processing text 149/850\n",
            "Processing text 150/850\n",
            "Processing text 151/850\n",
            "Processing text 152/850\n",
            "Processing text 153/850\n",
            "Processing text 154/850\n",
            "Processing text 155/850\n",
            "Processing text 156/850\n",
            "Processing text 157/850\n",
            "Processing text 158/850\n",
            "Processing text 159/850\n",
            "Processing text 160/850\n",
            "Processing text 161/850\n",
            "Processing text 162/850\n",
            "Processing text 163/850\n",
            "Processing text 164/850\n",
            "Processing text 165/850\n",
            "Processing text 166/850\n",
            "Processing text 167/850\n",
            "Processing text 168/850\n",
            "Processing text 169/850\n",
            "Processing text 170/850\n",
            "Processing text 171/850\n",
            "Processing text 172/850\n",
            "Processing text 173/850\n",
            "Processing text 174/850\n",
            "Processing text 175/850\n",
            "Processing text 176/850\n",
            "Processing text 177/850\n",
            "Processing text 178/850\n",
            "Processing text 179/850\n",
            "Processing text 180/850\n",
            "Processing text 181/850\n",
            "Processing text 182/850\n",
            "Processing text 183/850\n",
            "Processing text 184/850\n",
            "Processing text 185/850\n",
            "Processing text 186/850\n",
            "Processing text 187/850\n",
            "Processing text 188/850\n",
            "Processing text 189/850\n",
            "Processing text 190/850\n",
            "Processing text 191/850\n",
            "Processing text 192/850\n",
            "Processing text 193/850\n",
            "Processing text 194/850\n",
            "Processing text 195/850\n",
            "Processing text 196/850\n",
            "Processing text 197/850\n",
            "Processing text 198/850\n",
            "Processing text 199/850\n",
            "Processing text 200/850\n",
            "Processing text 201/850\n",
            "Processing text 202/850\n",
            "Processing text 203/850\n",
            "Processing text 204/850\n",
            "Processing text 205/850\n",
            "Processing text 206/850\n",
            "Processing text 207/850\n",
            "Processing text 208/850\n",
            "Processing text 209/850\n",
            "Processing text 210/850\n",
            "Processing text 211/850\n",
            "Processing text 212/850\n",
            "Processing text 213/850\n",
            "Processing text 214/850\n",
            "Processing text 215/850\n",
            "Processing text 216/850\n",
            "Processing text 217/850\n",
            "Processing text 218/850\n",
            "Processing text 219/850\n",
            "Processing text 220/850\n",
            "Processing text 221/850\n",
            "Processing text 222/850\n",
            "Processing text 223/850\n",
            "Processing text 224/850\n",
            "Processing text 225/850\n",
            "Processing text 226/850\n",
            "Processing text 227/850\n",
            "Processing text 228/850\n",
            "Processing text 229/850\n",
            "Processing text 230/850\n",
            "Processing text 231/850\n",
            "Processing text 232/850\n",
            "Processing text 233/850\n",
            "Processing text 234/850\n",
            "Processing text 235/850\n",
            "Processing text 236/850\n",
            "Processing text 237/850\n",
            "Processing text 238/850\n",
            "Processing text 239/850\n",
            "Processing text 240/850\n",
            "Processing text 241/850\n",
            "Processing text 242/850\n",
            "Processing text 243/850\n",
            "Processing text 244/850\n",
            "Processing text 245/850\n",
            "Processing text 246/850\n",
            "Processing text 247/850\n",
            "Processing text 248/850\n",
            "Processing text 249/850\n",
            "Processing text 250/850\n",
            "Processing text 251/850\n",
            "Processing text 252/850\n",
            "Processing text 253/850\n",
            "Processing text 254/850\n",
            "Processing text 255/850\n",
            "Processing text 256/850\n",
            "Processing text 257/850\n",
            "Processing text 258/850\n",
            "Processing text 259/850\n",
            "Processing text 260/850\n",
            "Processing text 261/850\n",
            "Processing text 262/850\n",
            "Processing text 263/850\n",
            "Processing text 264/850\n",
            "Processing text 265/850\n",
            "Processing text 266/850\n",
            "Processing text 267/850\n",
            "Processing text 268/850\n",
            "Processing text 269/850\n",
            "Processing text 270/850\n",
            "Processing text 271/850\n",
            "Processing text 272/850\n",
            "Processing text 273/850\n",
            "Processing text 274/850\n",
            "Processing text 275/850\n",
            "Processing text 276/850\n",
            "Processing text 277/850\n",
            "Processing text 278/850\n",
            "Processing text 279/850\n",
            "Processing text 280/850\n",
            "Processing text 281/850\n",
            "Processing text 282/850\n",
            "Processing text 283/850\n",
            "Processing text 284/850\n",
            "Processing text 285/850\n",
            "Processing text 286/850\n",
            "Processing text 287/850\n",
            "Processing text 288/850\n",
            "Processing text 289/850\n",
            "Processing text 290/850\n",
            "Processing text 291/850\n",
            "Processing text 292/850\n",
            "Processing text 293/850\n",
            "Processing text 294/850\n",
            "Processing text 295/850\n",
            "Processing text 296/850\n",
            "Processing text 297/850\n",
            "Processing text 298/850\n",
            "Processing text 299/850\n",
            "Processing text 300/850\n",
            "Processing text 301/850\n",
            "Processing text 302/850\n",
            "Processing text 303/850\n",
            "Processing text 304/850\n",
            "Processing text 305/850\n",
            "Processing text 306/850\n",
            "Processing text 307/850\n",
            "Processing text 308/850\n",
            "Processing text 309/850\n",
            "Processing text 310/850\n",
            "Processing text 311/850\n",
            "Processing text 312/850\n",
            "Processing text 313/850\n",
            "Processing text 314/850\n",
            "Processing text 315/850\n",
            "Processing text 316/850\n",
            "Processing text 317/850\n",
            "Processing text 318/850\n",
            "Processing text 319/850\n",
            "Processing text 320/850\n",
            "Processing text 321/850\n",
            "Processing text 322/850\n",
            "Processing text 323/850\n",
            "Processing text 324/850\n",
            "Processing text 325/850\n",
            "Processing text 326/850\n",
            "Processing text 327/850\n",
            "Processing text 328/850\n",
            "Processing text 329/850\n",
            "Processing text 330/850\n",
            "Processing text 331/850\n",
            "Processing text 332/850\n",
            "Processing text 333/850\n",
            "Processing text 334/850\n",
            "Processing text 335/850\n",
            "Processing text 336/850\n",
            "Processing text 337/850\n",
            "Processing text 338/850\n",
            "Processing text 339/850\n",
            "Processing text 340/850\n",
            "Processing text 341/850\n",
            "Processing text 342/850\n",
            "Processing text 343/850\n",
            "Processing text 344/850\n",
            "Processing text 345/850\n",
            "Processing text 346/850\n",
            "Processing text 347/850\n",
            "Processing text 348/850\n",
            "Processing text 349/850\n",
            "Processing text 350/850\n",
            "Processing text 351/850\n",
            "Processing text 352/850\n",
            "Processing text 353/850\n",
            "Processing text 354/850\n",
            "Processing text 355/850\n",
            "Processing text 356/850\n",
            "Processing text 357/850\n",
            "Processing text 358/850\n",
            "Processing text 359/850\n",
            "Processing text 360/850\n",
            "Processing text 361/850\n",
            "Processing text 362/850\n",
            "Processing text 363/850\n",
            "Processing text 364/850\n",
            "Processing text 365/850\n",
            "Processing text 366/850\n",
            "Processing text 367/850\n",
            "Processing text 368/850\n",
            "Processing text 369/850\n",
            "Processing text 370/850\n",
            "Processing text 371/850\n",
            "Processing text 372/850\n",
            "Processing text 373/850\n",
            "Processing text 374/850\n",
            "Processing text 375/850\n",
            "Processing text 376/850\n",
            "Processing text 377/850\n",
            "Processing text 378/850\n",
            "Processing text 379/850\n",
            "Processing text 380/850\n",
            "Processing text 381/850\n",
            "Processing text 382/850\n",
            "Processing text 383/850\n",
            "Processing text 384/850\n",
            "Processing text 385/850\n",
            "Processing text 386/850\n",
            "Processing text 387/850\n",
            "Processing text 388/850\n",
            "Processing text 389/850\n",
            "Processing text 390/850\n",
            "Processing text 391/850\n",
            "Processing text 392/850\n",
            "Processing text 393/850\n",
            "Processing text 394/850\n",
            "Processing text 395/850\n",
            "Processing text 396/850\n",
            "Processing text 397/850\n",
            "Processing text 398/850\n",
            "Processing text 399/850\n",
            "Processing text 400/850\n",
            "Processing text 401/850\n",
            "Processing text 402/850\n",
            "Processing text 403/850\n",
            "Processing text 404/850\n",
            "Processing text 405/850\n",
            "Processing text 406/850\n",
            "Processing text 407/850\n",
            "Processing text 408/850\n",
            "Processing text 409/850\n",
            "Processing text 410/850\n",
            "Processing text 411/850\n",
            "Processing text 412/850\n",
            "Processing text 413/850\n",
            "Processing text 414/850\n",
            "Processing text 415/850\n",
            "Processing text 416/850\n",
            "Processing text 417/850\n",
            "Processing text 418/850\n",
            "Processing text 419/850\n",
            "Processing text 420/850\n",
            "Processing text 421/850\n",
            "Processing text 422/850\n",
            "Processing text 423/850\n",
            "Processing text 424/850\n",
            "Processing text 425/850\n",
            "Processing text 426/850\n",
            "Processing text 427/850\n",
            "Processing text 428/850\n",
            "Processing text 429/850\n",
            "Processing text 430/850\n",
            "Processing text 431/850\n",
            "Processing text 432/850\n",
            "Processing text 433/850\n",
            "Processing text 434/850\n",
            "Processing text 435/850\n",
            "Processing text 436/850\n",
            "Processing text 437/850\n",
            "Processing text 438/850\n",
            "Processing text 439/850\n",
            "Processing text 440/850\n",
            "Processing text 441/850\n",
            "Processing text 442/850\n",
            "Processing text 443/850\n",
            "Processing text 444/850\n",
            "Processing text 445/850\n",
            "Processing text 446/850\n",
            "Processing text 447/850\n",
            "Processing text 448/850\n",
            "Processing text 449/850\n",
            "Processing text 450/850\n",
            "Processing text 451/850\n",
            "Processing text 452/850\n",
            "Processing text 453/850\n",
            "Processing text 454/850\n",
            "Processing text 455/850\n",
            "Processing text 456/850\n",
            "Processing text 457/850\n",
            "Processing text 458/850\n",
            "Processing text 459/850\n",
            "Processing text 460/850\n",
            "Processing text 461/850\n",
            "Processing text 462/850\n",
            "Processing text 463/850\n",
            "Processing text 464/850\n",
            "Processing text 465/850\n",
            "Processing text 466/850\n",
            "Processing text 467/850\n",
            "Processing text 468/850\n",
            "Processing text 469/850\n",
            "Processing text 470/850\n",
            "Processing text 471/850\n",
            "Processing text 472/850\n",
            "Processing text 473/850\n",
            "Processing text 474/850\n",
            "Processing text 475/850\n",
            "Processing text 476/850\n",
            "Processing text 477/850\n",
            "Processing text 478/850\n",
            "Processing text 479/850\n",
            "Processing text 480/850\n",
            "Processing text 481/850\n",
            "Processing text 482/850\n",
            "Processing text 483/850\n",
            "Processing text 484/850\n",
            "Processing text 485/850\n",
            "Processing text 486/850\n",
            "Processing text 487/850\n",
            "Processing text 488/850\n",
            "Processing text 489/850\n",
            "Processing text 490/850\n",
            "Processing text 491/850\n",
            "Processing text 492/850\n",
            "Processing text 493/850\n",
            "Processing text 494/850\n",
            "Processing text 495/850\n",
            "Processing text 496/850\n",
            "Processing text 497/850\n",
            "Processing text 498/850\n",
            "Processing text 499/850\n",
            "Processing text 500/850\n",
            "Processing text 501/850\n",
            "Processing text 502/850\n",
            "Processing text 503/850\n",
            "Processing text 504/850\n",
            "Processing text 505/850\n",
            "Processing text 506/850\n",
            "Processing text 507/850\n",
            "Processing text 508/850\n",
            "Processing text 509/850\n",
            "Processing text 510/850\n",
            "Processing text 511/850\n",
            "Processing text 512/850\n",
            "Processing text 513/850\n",
            "Processing text 514/850\n",
            "Processing text 515/850\n",
            "Processing text 516/850\n",
            "Processing text 517/850\n",
            "Processing text 518/850\n",
            "Processing text 519/850\n",
            "Processing text 520/850\n",
            "Processing text 521/850\n",
            "Processing text 522/850\n",
            "Processing text 523/850\n",
            "Processing text 524/850\n",
            "Processing text 525/850\n",
            "Processing text 526/850\n",
            "Processing text 527/850\n",
            "Processing text 528/850\n",
            "Processing text 529/850\n",
            "Processing text 530/850\n",
            "Processing text 531/850\n",
            "Processing text 532/850\n",
            "Processing text 533/850\n",
            "Processing text 534/850\n",
            "Processing text 535/850\n",
            "Processing text 536/850\n",
            "Processing text 537/850\n",
            "Processing text 538/850\n",
            "Processing text 539/850\n",
            "Processing text 540/850\n",
            "Processing text 541/850\n",
            "Processing text 542/850\n",
            "Processing text 543/850\n",
            "Processing text 544/850\n",
            "Processing text 545/850\n",
            "Processing text 546/850\n",
            "Processing text 547/850\n",
            "Processing text 548/850\n",
            "Processing text 549/850\n",
            "Processing text 550/850\n",
            "Processing text 551/850\n",
            "Processing text 552/850\n",
            "Processing text 553/850\n",
            "Processing text 554/850\n",
            "Processing text 555/850\n",
            "Processing text 556/850\n",
            "Processing text 557/850\n",
            "Processing text 558/850\n",
            "Processing text 559/850\n",
            "Processing text 560/850\n",
            "Processing text 561/850\n",
            "Processing text 562/850\n",
            "Processing text 563/850\n",
            "Processing text 564/850\n",
            "Processing text 565/850\n",
            "Processing text 566/850\n",
            "Processing text 567/850\n",
            "Processing text 568/850\n",
            "Processing text 569/850\n",
            "Processing text 570/850\n",
            "Processing text 571/850\n",
            "Processing text 572/850\n",
            "Processing text 573/850\n",
            "Processing text 574/850\n",
            "Processing text 575/850\n",
            "Processing text 576/850\n",
            "Processing text 577/850\n",
            "Processing text 578/850\n",
            "Processing text 579/850\n",
            "Processing text 580/850\n",
            "Processing text 581/850\n",
            "Processing text 582/850\n",
            "Processing text 583/850\n",
            "Processing text 584/850\n",
            "Processing text 585/850\n",
            "Processing text 586/850\n",
            "Processing text 587/850\n",
            "Processing text 588/850\n",
            "Processing text 589/850\n",
            "Processing text 590/850\n",
            "Processing text 591/850\n",
            "Processing text 592/850\n",
            "Processing text 593/850\n",
            "Processing text 594/850\n",
            "Processing text 595/850\n",
            "Processing text 596/850\n",
            "Processing text 597/850\n",
            "Processing text 598/850\n",
            "Processing text 599/850\n",
            "Processing text 600/850\n",
            "Processing text 601/850\n",
            "Processing text 602/850\n",
            "Processing text 603/850\n",
            "Processing text 604/850\n",
            "Processing text 605/850\n",
            "Processing text 606/850\n",
            "Processing text 607/850\n",
            "Processing text 608/850\n",
            "Processing text 609/850\n",
            "Processing text 610/850\n",
            "Processing text 611/850\n",
            "Processing text 612/850\n",
            "Processing text 613/850\n",
            "Processing text 614/850\n",
            "Processing text 615/850\n",
            "Processing text 616/850\n",
            "Processing text 617/850\n",
            "Processing text 618/850\n",
            "Processing text 619/850\n",
            "Processing text 620/850\n",
            "Processing text 621/850\n",
            "Processing text 622/850\n",
            "Processing text 623/850\n",
            "Processing text 624/850\n",
            "Processing text 625/850\n",
            "Processing text 626/850\n",
            "Processing text 627/850\n",
            "Processing text 628/850\n",
            "Processing text 629/850\n",
            "Processing text 630/850\n",
            "Processing text 631/850\n",
            "Processing text 632/850\n",
            "Processing text 633/850\n",
            "Processing text 634/850\n",
            "Processing text 635/850\n",
            "Processing text 636/850\n",
            "Processing text 637/850\n",
            "Processing text 638/850\n",
            "Processing text 639/850\n",
            "Processing text 640/850\n",
            "Processing text 641/850\n",
            "Processing text 642/850\n",
            "Processing text 643/850\n",
            "Processing text 644/850\n",
            "Processing text 645/850\n",
            "Processing text 646/850\n",
            "Processing text 647/850\n",
            "Processing text 648/850\n",
            "Processing text 649/850\n",
            "Processing text 650/850\n",
            "Processing text 651/850\n",
            "Processing text 652/850\n",
            "Processing text 653/850\n",
            "Processing text 654/850\n",
            "Processing text 655/850\n",
            "Processing text 656/850\n",
            "Processing text 657/850\n",
            "Processing text 658/850\n",
            "Processing text 659/850\n",
            "Processing text 660/850\n",
            "Processing text 661/850\n",
            "Processing text 662/850\n",
            "Processing text 663/850\n",
            "Processing text 664/850\n",
            "Processing text 665/850\n",
            "Processing text 666/850\n",
            "Processing text 667/850\n",
            "Processing text 668/850\n",
            "Processing text 669/850\n",
            "Processing text 670/850\n",
            "Processing text 671/850\n",
            "Processing text 672/850\n",
            "Processing text 673/850\n",
            "Processing text 674/850\n",
            "Processing text 675/850\n",
            "Processing text 676/850\n",
            "Processing text 677/850\n",
            "Processing text 678/850\n",
            "Processing text 679/850\n",
            "Processing text 680/850\n",
            "Processing text 681/850\n",
            "Processing text 682/850\n",
            "Processing text 683/850\n",
            "Processing text 684/850\n",
            "Processing text 685/850\n",
            "Processing text 686/850\n",
            "Processing text 687/850\n",
            "Processing text 688/850\n",
            "Processing text 689/850\n",
            "Processing text 690/850\n",
            "Processing text 691/850\n",
            "Processing text 692/850\n",
            "Processing text 693/850\n",
            "Processing text 694/850\n",
            "Processing text 695/850\n",
            "Processing text 696/850\n",
            "Processing text 697/850\n",
            "Processing text 698/850\n",
            "Processing text 699/850\n",
            "Processing text 700/850\n",
            "Processing text 701/850\n",
            "Processing text 702/850\n",
            "Processing text 703/850\n",
            "Processing text 704/850\n",
            "Processing text 705/850\n",
            "Processing text 706/850\n",
            "Processing text 707/850\n",
            "Processing text 708/850\n",
            "Processing text 709/850\n",
            "Processing text 710/850\n",
            "Processing text 711/850\n",
            "Processing text 712/850\n",
            "Processing text 713/850\n",
            "Processing text 714/850\n",
            "Processing text 715/850\n",
            "Processing text 716/850\n",
            "Processing text 717/850\n",
            "Processing text 718/850\n",
            "Processing text 719/850\n",
            "Processing text 720/850\n",
            "Processing text 721/850\n",
            "Processing text 722/850\n",
            "Processing text 723/850\n",
            "Processing text 724/850\n",
            "Processing text 725/850\n",
            "Processing text 726/850\n",
            "Processing text 727/850\n",
            "Processing text 728/850\n",
            "Processing text 729/850\n",
            "Processing text 730/850\n",
            "Processing text 731/850\n",
            "Processing text 732/850\n",
            "Processing text 733/850\n",
            "Processing text 734/850\n",
            "Processing text 735/850\n",
            "Processing text 736/850\n",
            "Processing text 737/850\n",
            "Processing text 738/850\n",
            "Processing text 739/850\n",
            "Processing text 740/850\n",
            "Processing text 741/850\n",
            "Processing text 742/850\n",
            "Processing text 743/850\n",
            "Processing text 744/850\n",
            "Processing text 745/850\n",
            "Processing text 746/850\n",
            "Processing text 747/850\n",
            "Processing text 748/850\n",
            "Processing text 749/850\n",
            "Processing text 750/850\n",
            "Processing text 751/850\n",
            "Processing text 752/850\n",
            "Processing text 753/850\n",
            "Processing text 754/850\n",
            "Processing text 755/850\n",
            "Processing text 756/850\n",
            "Processing text 757/850\n",
            "Processing text 758/850\n",
            "Processing text 759/850\n",
            "Processing text 760/850\n",
            "Processing text 761/850\n",
            "Processing text 762/850\n",
            "Processing text 763/850\n",
            "Processing text 764/850\n",
            "Processing text 765/850\n",
            "Processing text 766/850\n",
            "Processing text 767/850\n",
            "Processing text 768/850\n",
            "Processing text 769/850\n",
            "Processing text 770/850\n",
            "Processing text 771/850\n",
            "Processing text 772/850\n",
            "Processing text 773/850\n",
            "Processing text 774/850\n",
            "Processing text 775/850\n",
            "Processing text 776/850\n",
            "Processing text 777/850\n",
            "Processing text 778/850\n",
            "Processing text 779/850\n",
            "Processing text 780/850\n",
            "Processing text 781/850\n",
            "Processing text 782/850\n",
            "Processing text 783/850\n",
            "Processing text 784/850\n",
            "Processing text 785/850\n",
            "Processing text 786/850\n",
            "Processing text 787/850\n",
            "Processing text 788/850\n",
            "Processing text 789/850\n",
            "Processing text 790/850\n",
            "Processing text 791/850\n",
            "Processing text 792/850\n",
            "Processing text 793/850\n",
            "Processing text 794/850\n",
            "Processing text 795/850\n",
            "Processing text 796/850\n",
            "Processing text 797/850\n",
            "Processing text 798/850\n",
            "Processing text 799/850\n",
            "Processing text 800/850\n",
            "Processing text 801/850\n",
            "Processing text 802/850\n",
            "Processing text 803/850\n",
            "Processing text 804/850\n",
            "Processing text 805/850\n",
            "Processing text 806/850\n",
            "Processing text 807/850\n",
            "Processing text 808/850\n",
            "Processing text 809/850\n",
            "Processing text 810/850\n",
            "Processing text 811/850\n",
            "Processing text 812/850\n",
            "Processing text 813/850\n",
            "Processing text 814/850\n",
            "Processing text 815/850\n",
            "Processing text 816/850\n",
            "Processing text 817/850\n",
            "Processing text 818/850\n",
            "Processing text 819/850\n",
            "Processing text 820/850\n",
            "Processing text 821/850\n",
            "Processing text 822/850\n",
            "Processing text 823/850\n",
            "Processing text 824/850\n",
            "Processing text 825/850\n",
            "Processing text 826/850\n",
            "Processing text 827/850\n",
            "Processing text 828/850\n",
            "Processing text 829/850\n",
            "Processing text 830/850\n",
            "Processing text 831/850\n",
            "Processing text 832/850\n",
            "Processing text 833/850\n",
            "Processing text 834/850\n",
            "Processing text 835/850\n",
            "Processing text 836/850\n",
            "Processing text 837/850\n",
            "Processing text 838/850\n",
            "Processing text 839/850\n",
            "Processing text 840/850\n",
            "Processing text 841/850\n",
            "Processing text 842/850\n",
            "Processing text 843/850\n",
            "Processing text 844/850\n",
            "Processing text 845/850\n",
            "Processing text 846/850\n",
            "Processing text 847/850\n",
            "Processing text 848/850\n",
            "Processing text 849/850\n",
            "Processing text 850/850\n",
            "Finished predicting sentiment for all texts.\n",
            "Saving the predicted sentiments to enhanced_top_18_stocks.json...\n",
            "All processes complete. Data with sentiment predictions saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"Starting the sentiment analysis script...\")\n",
        "\n",
        "# Load the trained BERT model\n",
        "model_name = 'bert-base-uncased'\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "print(\"Loading the trained BERT model...\")\n",
        "bert_model.load_weights('bert_model')  # Use the correct folder name where your model's weights are saved\n",
        "\n",
        "# Load data from the output file\n",
        "print(\"Loading data from the JSON file...\")\n",
        "with open('top_18_stocks.json', 'r') as file:  # Adjust the path if necessary\n",
        "    data = json.load(file)\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Initialize BERT tokenizer\n",
        "print(\"Initializing the BERT tokenizer...\")\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Function to preprocess and predict sentiment for each summary\n",
        "def predict_sentiment(texts, tokenizer, model, max_length=128):\n",
        "    predictions = []\n",
        "    print(\"Starting sentiment prediction for each text...\")\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        print(f\"Processing text {i+1}/{len(texts)}\")\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"tf\",\n",
        "            truncation=True\n",
        "        )\n",
        "        outputs = model(inputs)\n",
        "        probs = tf.nn.softmax(outputs.logits, axis=-1)\n",
        "        predicted_label = tf.argmax(probs, axis=-1).numpy()[0]\n",
        "        predictions.append(predicted_label)\n",
        "\n",
        "    print(\"Finished predicting sentiment for all texts.\")\n",
        "    return predictions\n",
        "\n",
        "# Predict sentiment for each summary in the DataFrame\n",
        "print(\"Running predictions on summaries...\")\n",
        "sentiments = predict_sentiment(df['summary'].tolist(), tokenizer, bert_model)\n",
        "\n",
        "# Map the numeric predictions back to sentiment labels\n",
        "label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "df['predicted_sentiment'] = [label_map[label] for label in sentiments]\n",
        "\n",
        "# Save the results to a new JSON file\n",
        "output_file_path = 'enhanced_top_18_stocks.json'  # Adjust the path if necessary\n",
        "print(f\"Saving the predicted sentiments to {output_file_path}...\")\n",
        "df.to_json(output_file_path, orient='records', lines=True)\n",
        "\n",
        "print(\"All processes complete. Data with sentiment predictions saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ld-jhiMLatf9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "json_file_path = 'enhanced_top_18_stocks.json'  # Replace with your JSON file path\n",
        "data = pd.read_json(json_file_path, lines=True)\n",
        "\n",
        "csv_file_path = 'enhanced_top_18_stocks.csv'  # Replace with your desired CSV file path\n",
        "data.to_csv(csv_file_path, index=False)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
